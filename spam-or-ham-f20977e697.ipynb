{"cells":[{"metadata":{"_uuid":"f6af4f9cfe6956198997ba048b90cbf0d01b37ab"},"cell_type":"markdown","source":"# PMR3508 - Aprendizado de Máquina e Reconhecimento de Padrões\n**Tarefa 2 - Aprendendo a lidar com o classificador Naive Bayes**\n\nHASH: f20977e697\n"},{"metadata":{"_uuid":"5f821272691575e363a63ea0c27adbc462faba7c"},"cell_type":"markdown","source":"## Table of Contents\n\n- [Introduction](#introduction)\n- [Initial Data Exploration](#initial)\n- [Naive Bayes](#classifier)\n- [K-nearest neighbors](#classifier)\n- [Conclusions](#conclusions)\n- [References](#references) \n"},{"metadata":{"_uuid":"8158b6f751a20916890ad06a1d089d853f1d89b7"},"cell_type":"markdown","source":"## 1. Introduction\n\n  This spam classifier is implemented by Naive Bayes Model, a simple but very efficient solution in spam classification problem. In brief, Naive Bayes remains a popular (baseline) method for text categorization, the problem of judging documents as belonging to one category or the other (such as spam or legitimate, sports or politics, etc.) with word frequencies as the features. In this kernel we will explore some data about spam and ham, compare two methods of classifications and discuss the differences between them."},{"metadata":{"_uuid":"18bf7770591456b29a264c6bfc9ae850edbca32b"},"cell_type":"markdown","source":"### Environment Setting\n\nBefore we start we need set up the environment, please make sure those packages are installed in your computer when you copy this code to the local file."},{"metadata":{"trusted":false,"_uuid":"d06365eaaff3953697a63854d97fb92df5eb38be"},"cell_type":"code","source":"#Importing stuff that I will need in this notebook\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import feature_extraction, model_selection, naive_bayes, metrics\nfrom collections import Counter\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,fbeta_score\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0545623fb48d07f29c9ee2fd3bd0d7dec399d453"},"cell_type":"markdown","source":"### Read Data File\n\nThe first task is to read data from .csv file. Most of the features are frequency of some especif word."},{"metadata":{"trusted":false,"_uuid":"3b62ba7dfe602ddcaa37691d37a4c8e952baeaae"},"cell_type":"code","source":"train = pd.read_csv( \"../input/spambase/train_data.csv\")\ntestTOsubmit = pd.read_csv( \"../input/spambase/test_features.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d47f149188bc5a935eb566c1ddb11d80da34c0fa"},"cell_type":"code","source":"print (\"train size: \", train.shape)\nprint (\"test size: \", testTOsubmit.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5195417cc23b7fcf44b7480fb9b747b68653c029"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecf128812c0dda179c64135cd257cb2cca2a3cfa"},"cell_type":"markdown","source":" - As we can see below, the features are mostly a frequency of some especific word. Also, the database does not have missing data (which is good for a better analysis)"},{"metadata":{"trusted":false,"_uuid":"2adeb792e7ad608b8008c410a08e04cf4ba61179"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d738f45935b2a0495295ca5a495379f5ba0519d4"},"cell_type":"markdown","source":"## 2. Initial data exploration\n\n In this section, we will explore the data in order to understand what each of them means in the database and how they relate to the target variable."},{"metadata":{"trusted":false,"_uuid":"456691d55f77a22e500b7751db596924e0b575a4"},"cell_type":"code","source":"# Plot a graph with the proportions of spam or ham in our data\n\ncount_Class=pd.value_counts(train[\"ham\"], sort= True)\ncount_Class.plot(kind= 'bar', color= [\"green\", \"red\"])\nplt.title('Is it a ham?')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"673bc2bed618a83edc310c54ce985a2052627476"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7a00d1f5bb492a8f09b1d1784777983dd2d7f0d"},"cell_type":"markdown","source":" - As we can see above, some words have more variation on average than others (which means that they are less common in email texts)."},{"metadata":{"_uuid":"8299c556c2bbcc1fcfa9f9fa63cc2c2434156197"},"cell_type":"markdown","source":"### Separate in groups (for fequency analysis)\n \n - Firstly, we are going to focus on the management of the frequency variables and them relations. So, let's drop the other columns and separate the data in two groups (one with spam emails and the other with the non-spam)."},{"metadata":{"trusted":false,"_uuid":"67ad7111d68b6a943e1a6308aa6b90606352375d"},"cell_type":"code","source":"# Drop the columns that is not about frquency\n\ntrain2 = train.drop(columns = ['capital_run_length_average', 'capital_run_length_longest', 'capital_run_length_total', 'Id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"94d510b582b33a08ad187dc7fe464419769c660d"},"cell_type":"code","source":"# Separate the ham and spam for better analisys\n\nHAMdata = train2[train2['ham']==True]\nSPAMdata = train2[train2['ham']==False]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"1074fd2c23964f073ee5554ce67134d719ddfaf4"},"cell_type":"code","source":"# Drop the column that classify if it is ham or not\n\nHAMdata.drop(columns = ['ham'])\nSPAMdata.drop(columns = ['ham'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"feeb82cc69987bb041d0b95dfaddca1657005632"},"cell_type":"markdown","source":"### Find useful informations\n\n- Secondly, we will try to find out some basic info and limitations of our classifier."},{"metadata":{"trusted":false,"_uuid":"2e55aeffa1d4c387dade0ed989faf42cde68dcf6"},"cell_type":"code","source":"# Discovering some information about our data that can be useful\n\nHAMline = HAMdata.shape[0]\nSPAMline = SPAMdata.shape[0]\nALLlines = SPAMline + HAMline\nSPAMperc = SPAMline * 100 / ALLlines\nHAMperc = HAMline * 100 / ALLlines\n\nprint('     Useful Information')\nprint()\nprint (\"HAM data size: \", HAMdata.shape)\nprint (\"SPAM data size: \", SPAMdata.shape)\nprint(\"HAM percentage: \", HAMperc)\nprint(\"SPAM percentage: \", SPAMperc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0c467ad885b11d696f5c0a79e919bf5569b6f49"},"cell_type":"markdown","source":" - We need to find a way to our program get a classifier with an error less than 38% (because, in the worst case of telling that every email is HAM, we have 38% of error). Furthermore,  we can conclude the proportion between the emails that are or not spam and realize that our database is reasonably large (which is good for machine learning)."},{"metadata":{"_uuid":"f53405e405c78cdeb8eb87c18cebfde1a405174c"},"cell_type":"markdown","source":"### Comparing frequency of each word or char in spam and non-spam emails\n\n- Now, let's see the frequency of each word or char in spam and non-spam emails"},{"metadata":{"trusted":false,"_uuid":"35ff6de9758020a2d5e05022952df8e9543fdca0"},"cell_type":"code","source":"# Set the frequency of words and chars in the non-spam email\n\n\nsums1 = HAMdata.select_dtypes(pd.np.number).sum().rename('total')\nprint('Frequency of words and chars in the non-spam email')\nprint()\nprint(sums1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5172f9a1cdcefe49a118ba3fcd5d743a1cb16496"},"cell_type":"code","source":"# Set the frequency of words and chars in the spam email\n\nsums2 = SPAMdata.select_dtypes(pd.np.number).sum().rename('total')\nprint('Frequency of words and chars in the spam email')\nprint()\nprint(sums2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c13c2d881143f902cffe183bcd6832ffc458ebee"},"cell_type":"code","source":"# Plot the graph of more frequent words in non-spam messages\n\nsums1.plot.bar(legend = False)\nplt.title('More frequent words in non-spam messages')\nplt.xlabel('Words or Chars')\nplt.ylabel('Numbers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9c600494d01c07214d1002dbd039cc964aabfe8f"},"cell_type":"code","source":"# Plot the graph of more frequent words in spam messages\n\nsums2.plot.bar(legend = False)\nplt.title('More frequent words in spam messages')\nplt.xlabel('Words or Chars')\nplt.ylabel('Numbers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ee24b865429382583ce9af3f0940828fc78fbd1"},"cell_type":"markdown","source":" - As we can see, the frequency of \"you\" in both classifications of emails are very high, but, in terms of \"your\", the spam messages have this word more frequently. So, we will drop only the frequency of \"you\" (it is not meaningful)."},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"624397cb94031992c111eb1c141c3d209bfb2ec7"},"cell_type":"code","source":"train.drop(columns = ['word_freq_you'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75bba5fa9fce49f468329da7da2424f423773f13"},"cell_type":"markdown","source":" - Moreover, we can notice that \"hp\", \"hpl\" and \"george\" are very frequent in non-spam messages (compared to the frequency in spam messages), maybe, this information will be useful posteriorly."},{"metadata":{"_uuid":"8bc141e9d87ca5f24996a2e83e1cf5c5f0cab07e"},"cell_type":"markdown","source":"### Comparing frequency between each word or char in spam and non-spam emails\n\n- Now, let's see the difference between frequency of each word or char in spam and non-spam emails. Thus, we can infer which words or chars are more important in our clasification"},{"metadata":{"trusted":false,"_uuid":"d4a5bd61328217ef9af55997b0e8a0db2d5ad3dd"},"cell_type":"code","source":"# Set the frequency of words and chars in each non-spam email\n\nsums1FRACTION = sums1 / HAMline\n\n# Set the frequency of words and chars in each spam email\n\nsums2FRACTION = sums2 / SPAMline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db22abbd4e5a3a9fd4e2f44bc96e4e595c8253b5"},"cell_type":"markdown","source":" - We will subtract a fraction for the other. The larger the number module, the more relevant this word or char is to our classifier. In addition, positive numbers indicate that the frequency is higher in spam emails, while negatives indicate a higher frequency in ham emails."},{"metadata":{"trusted":false,"_uuid":"f28580de7dac2f87f62c3d888a69a29f42601d5f"},"cell_type":"code","source":"# Set the difference between frequencies of words and chars\n\ndifSums = sums2FRACTION - sums1FRACTION\nprint(difSums)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"67a6872706fa61cf1adc411eac835406f006e150"},"cell_type":"code","source":"# Plot the graph of the difference between frequencies of words and chars\n\ndifSums.plot.bar(legend = False)\nplt.title('Difference between frequencies of words and chars')\nplt.xlabel('Words or Chars')\nplt.ylabel('Numbers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ca0377da9786f4b81e6564f2a8425e07e7ed031"},"cell_type":"markdown","source":" - As we can see above, a lot of information can be concluded through the graphic. The chars (\"$\" and \"#\") have a big difference in your frequencies in spam or not spam emails, so, you might infer that emails with those chars are spam. Moreover, the word \"free\" has the same frequency characteristics as the chars, so we can think the same way. The words \"you\" and \"your\" also have a high difference between frequency, but, unlike the words quoted above, these are very common words in talks/speeches (they are known as stop words in text classifiers), so, they will appear in non-spam emails (with a much lower frequency).\n - The same idea described in this previous paragraph can be applied to frequencies with negative numbers (and large module numbers). The emails that appear words like \"hp\", \"hpl\" and \"george\" probably are non-spam emails. \n - It is worth pointing out that we are concluding and supposing ideas from the data and this discussion does not replace an math classifier."},{"metadata":{"_uuid":"41bb4905f61ed309726b994d6b465d05e70190a2"},"cell_type":"markdown","source":"### Separate in groups (for length analysis)\n \n - In this part, we are going to focus on the management of lenght variables and them relations. So, let's drop the other columns and separate the data in two groups (one with spam emails and the other with the non-spam)."},{"metadata":{"trusted":false,"_uuid":"7243c17c2a26056e3a5946e5eda000e9670f4b35"},"cell_type":"code","source":"# Drop the columns that is about frquency and the Id column\n\ncolumns = ['capital_run_length_average', 'capital_run_length_longest', 'capital_run_length_total', 'ham']\ntrain3 = pd.DataFrame(train, columns=columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3d001b4444a7a81d6adbdbfc581cf4801f2819c1"},"cell_type":"code","source":"# Separate the ham and spam for better analisys\n\nHAMcarac = train3[train3['ham']==True]\nSPAMcarac = train3[train3['ham']==False]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"0c2b2b773181a6e88d41e23203a46621e985e64b"},"cell_type":"code","source":"# Drop the column that classify if it is ham or not\n\nHAMcarac.drop(columns = ['ham'])\nSPAMcarac.drop(columns = ['ham'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee231f52f2901095078ea02b6da17c5f5944d595"},"cell_type":"markdown","source":"### Comparing types of length between spam and non-spam emails\n\n- In this section, we will explore the data related to the lenght (of words, email...). As you can see, we will increase a method similar to the one used above."},{"metadata":{"trusted":false,"_uuid":"ec681ba25259ad7d29c8692e482da428a3b96603"},"cell_type":"code","source":"# Set the rate of the types of length in the non-spam email\n\nsums3 = HAMcarac.select_dtypes(pd.np.number).sum().rename('total')\nsums3FRACTION = sums3 / HAMline\nprint('Rate of the types of length in the non-spam email')\nprint()\nprint(sums3FRACTION)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4eff21cecfed76ef9b2afc9c5645f9fb7ffeca91"},"cell_type":"code","source":"# Set the rate of the types of length in the spam email\n\nsums4 = SPAMcarac.select_dtypes(pd.np.number).sum().rename('total')\nsums4FRACTION = sums4 / SPAMline\nprint('Rate of the types of length in the spam email')\nprint()\nprint(sums4FRACTION)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"723647f4d214985dc17f5d4ff65286d03db1ae6c"},"cell_type":"markdown","source":" - As we can see above, the average length of uninterrupted sequences of capital letters (capital_run_length_average), the length of longest uninterrupted sequence of capital letters (capital_run_length_longest) and the total number of capital letters in the e-mail (capital_run_length_total) are commonly larger in spam emails. That is, spam emails tend to be more extensive and prolix than the non-spam emails."},{"metadata":{"_uuid":"47d17a9d4f46ad8e8517df8b3e4a1b7f3bd40c15"},"cell_type":"markdown","source":"## 3. Naive Bayes\n\n Now, we will choose the classifier that best fits the solution of the problem. And then, we will apply the classifier's performance assessment methods to make decisions about the classifier and the hyperparameters."},{"metadata":{"_uuid":"911b43dd6785435de1133b7c45819c2a60f693d8"},"cell_type":"markdown","source":"- Firstly, we will split our training data in four parts"},{"metadata":{"trusted":false,"_uuid":"ba3462019cfe7d7645d8e6c4168af39e8471ccb5"},"cell_type":"code","source":"#Spliting the data in target(output) and features(input)\n\noutputDATA = train['ham']\ninputDATA = train.drop(columns=['ham'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"68616711881ec8c5173b6f99a46d339c3a61ebae"},"cell_type":"code","source":"#Spliting the data in train output, train inputs, test output and test inputs\n\ndata_train, data_test, target_train, target_test = train_test_split(\n    inputDATA,\n    outputDATA,\n    random_state = 0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3c0a18109bfa67e5ed5d07e2bb5c58c5e1258be9"},"cell_type":"code","source":"#Creating the object pertaining to the Naive Bayes classifier for normal probability distribution.\n\ngnb = GaussianNB()\ngnb.fit(data_train, target_train)\n\n#Predicting with Naive Bayes classifier\n\npredictions = gnb.predict(data_train)\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"565d6993fc970b527645eef11a3e8e23cef3b9a2"},"cell_type":"markdown","source":" - Here, we will plot our confusion matrix (In the field of machine learning and specifically the problem of statistical classification, a confusion matrix is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one."},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"3a6d60a0a4de14f52da08f82956ad9aa711bf6a1"},"cell_type":"code","source":"print ('Accuracy Score: ' + str(accuracy_score(target_train, predictions)))\nprint()\nprint (classification_report(target_train, predictions))\nprint (confusion_matrix(target_train, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"11d3ea47ea89ad0155ffabf0ca6bfa769cf940ec"},"cell_type":"code","source":"print('     Confusion Matrix')\nm_confusion_test = metrics.confusion_matrix(target_train, predictions)\npd.DataFrame(m_confusion_test, columns = ['Predicted Ham', 'Predicted Spam'],\n            index = ['Actual Ham', 'Actual Spam'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6830e0f1f363ee0a8e85751aa9c0198e8da74367"},"cell_type":"code","source":"# Setting some information that will be useful\n\nTP = m_confusion_test[0][0]\nTN = m_confusion_test[1][1]\nFP = m_confusion_test[1][0]\nFN = m_confusion_test[0][1]\nTrues = TP + TN\nFalses = FP + FN\nAll = TP + TN + FP + FN\nrecall = TP / (TP + FN)\nprecision = TP / (TP + FP)\nspecificity = TN / (TN + FP)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e2f3998e97c8ef6c7ecd830cb65961b8e4f509a"},"cell_type":"markdown","source":" - Then, let's calculate our F-score with beta equal to 3 (it means that we consider recall 3 times more important than precision). In statistical analysis of binary classification, the F3 score (also F-score or F-measure) is a measure of a test's accuracy. It is the harmonic average of the precision and recall, where an F3 score reaches its best value at 1 (perfect precision and recall) and worst at 0. "},{"metadata":{"trusted":false,"_uuid":"668b0646dbebaff5234964e6fab210def131d777"},"cell_type":"code","source":"F3_score = 10 * precision * recall / (9 * precision + recall)\nprint ('F3 Score: ' + str(F3_score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"676c01b77288d71eded9ff7fae5b6e6db8129bc7"},"cell_type":"markdown","source":" - In this step, we will plot the ROC Curve. (It is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varies. It's analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way to cost/benefit analysis of diagnostic decision making.)"},{"metadata":{"trusted":false,"_uuid":"3b80d494e2f6315ae3b79d006fed38e16f5a6c88"},"cell_type":"code","source":"# Calculate the Fpr and Tpr for all thresholds of the classification\n\nfpr, tpr, threshold = metrics.roc_curve(target_train, predictions)\nroc_auc = metrics.auc(fpr, tpr)\n\n# Ploting the (ROC Curve)\n\nplt.title('Receiver Operating Characteristic (The ROC cruve)')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f784e1a2fdb3cc626cf958115d386de9bb0c3ac4"},"cell_type":"markdown","source":"- Now we will search for the best CV, testing multiples numbers."},{"metadata":{"trusted":false,"_uuid":"4ac600a9c1651c4532d6761431c98871900f3a13"},"cell_type":"code","source":"CVlist = [5, 7, 10, 13, 15, 17, 20, 25, 30]\nfor i in CVlist:\n    scores = cross_val_score(gnb, data_train, target_train, cv=i)\n    strI = str(i)\n    mean = str(np.mean(scores))\n    print('Scores with CV equal to ' + strI)\n    print(scores)\n    print()\n    print('Mean of scores: ' + mean)\n    print()\n    print()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4aceb3f06eb3a640048b8df861e97973a7a5bedf"},"cell_type":"markdown","source":"- The best score found was with CV equal to 5"},{"metadata":{"_uuid":"e40010e2633addeb56fec99c89d87cf0b8285ca2"},"cell_type":"markdown","source":"## 4. K-nearest neighbors\n\n - In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space."},{"metadata":{"_uuid":"f546c11ab7e8a4471431d88e3aa3dda1769ac455"},"cell_type":"markdown","source":"- Firstly, we will split our training data in four parts"},{"metadata":{"trusted":false,"_uuid":"eb1c2a0ea91ce102a4d8f2868883478e9ddc789e"},"cell_type":"code","source":"#Spliting the data in target(output) and features(input)\n\noutputDATA = train['ham']\ninputDATA = train.drop(columns=['ham'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e8ffde2dde83b9e4cc27e112352145d3418c92db"},"cell_type":"code","source":"#Spliting the data in train output, train inputs, test output and test inputs\n\ndataVtrain, dataVtest, targetVtrain, targetVtest = train_test_split(\n    inputDATA,\n    outputDATA,\n    random_state = 0) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11c1301ff4e906feb53581617d103273c78a3859"},"cell_type":"markdown","source":" - Then, let's try some numbers to the K and CV and find the best performance"},{"metadata":{"trusted":false,"_uuid":"a611b34acb2a89c2311b0b4457efbd424c79f56d"},"cell_type":"code","source":"# Setting the K numbers and our datas of test and train\n\nneighbors = [3,5,7,9,13,15,20,25]\nXtrain = dataVtrain\nYtrain = targetVtrain\nXtest = dataVtest\nYtest = targetVtest","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8c930659542ac1f2e93141dfb3b9789f3a7ac1f8"},"cell_type":"code","source":"# Here, we define a function that test (with CV equal to 3) various K-nn and yours misclassification error\n\nprint('With CV = 3')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=3, scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"05aadc58d85a49ba667d223f119cbb72c7ea662d"},"cell_type":"code","source":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 3)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"37ef50bcbe7f234ce8457881d8e8824e9fa94623"},"cell_type":"code","source":"# Here, we define a function that test (with CV equal to 5) various K-nn and yours misclassification error\n\nprint('With CV = 5')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=5, scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"02c9d989e6512390cd64d9494d27fe07f0db6863"},"cell_type":"code","source":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 5)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"08fbc58f09afc05290175851e514234c52bce982"},"cell_type":"code","source":"# Here, we define a function that test (with CV equal to 7) various K-nn and yours misclassification error\n\nprint('With CV = 7')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=7, scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4afc54a7c95c8f2ab2e05bdf0e2daae2a9ac33f9"},"cell_type":"code","source":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 7)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"498851692eef0f8be7f8a728c3725d2873f17f59"},"cell_type":"code","source":"# Here, we define a function that test (with CV equal to 9) various K-nn and yours misclassification error\n\nprint('With CV = 9')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=9, scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3f441400a4871abc44b0e38cf05988fcd0391141"},"cell_type":"code","source":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 9)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0edd3214437fda602d8aa19d4a75a7b2dc2b6e08"},"cell_type":"code","source":"# Here, we define a function that test (with CV equal to 13) various K-nn and yours misclassification error\n\nprint('With CV = 13')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=13, scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"78a51f1cc62495a9e2829f38dddd9949aef338a1"},"cell_type":"code","source":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 13)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0d383c2a1fc0b4ac3fd62007fa8594cf4e0aaed9"},"cell_type":"code","source":"# Here, we define a function that test (with CV equal to 15) various K-nn and yours misclassification error\n\nprint('With CV = 15')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=15, scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"62611f12a90c757517297adf96d93ac581d9500c"},"cell_type":"code","source":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 15)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6d7c92da1b39a455d8fccc1d97fadd7e31319c26"},"cell_type":"code","source":"# Here, we define a function that test (with CV equal to 20) various K-nn and yours misclassification error\n\nprint('With CV = 20')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=20, scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"80308ca0219845f726b3e23dbc626376e78d9707"},"cell_type":"code","source":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 20)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a290935aa362f6aba76ecc98ab12fda8e195e682"},"cell_type":"code","source":"# Here, we define a function that test (with CV equal to 25) various K-nn and yours misclassification error\n\nprint('With CV = 25')\ncv_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, Xtrain, Ytrain, cv=25, scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \n    \nMSE = [1 - x for x in cv_scores]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d415ccbc56994b9ffe5ded65a8c273f466246758"},"cell_type":"code","source":"# Plot a graph that show us the perfomance of the K-nn with each number of neighbors (in this case, the CV is equal to 25)\n\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"888cde0e160cdac635468b47f121e91ceb3e8df8"},"cell_type":"markdown","source":" - We can conclude that our best perfomace with K-NN was with CV equal to 13 and the optimal number of neighbors is 20. So, let's use it and take more useful info of your classification."},{"metadata":{"trusted":false,"_uuid":"e8b89341234cce1f67613d6b8f8c1818ecc0ca3d"},"cell_type":"code","source":"# Set the K-nn with K=13 and CV=20\n\nknn = KNeighborsClassifier(n_neighbors=13)\nknn.fit(Xtrain,Ytrain)\nscores = cross_val_score(knn, Xtrain, Ytrain, cv=20)\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7763569846ffee167815861f8cf7546b73a9d290"},"cell_type":"code","source":"# Set the prediction\n\nYtrainPred = knn.predict(Xtrain)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abd17494701701486d904c87223dbbf4d0b0dddf"},"cell_type":"markdown","source":" - Here, we will plot our confusion matrix (In the field of machine learning and specifically the problem of statistical classification, a confusion matrix is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one."},{"metadata":{"trusted":false,"_uuid":"581ab906e28354eb9b3d288b9860da9994a2ed61"},"cell_type":"code","source":"print ('Accuracy Score: ' + str(accuracy_score(Ytrain, YtrainPred)))\nprint()\nprint (classification_report(Ytrain, YtrainPred))\nprint (confusion_matrix(Ytrain, YtrainPred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"667e138a9b18c93d7e129189bb82fd2df6b4b37c"},"cell_type":"code","source":"print('     Confusion Matrix')\nn_confusion_test = metrics.confusion_matrix(Ytrain, YtrainPred)\npd.DataFrame(n_confusion_test, columns = ['Predicted Ham', 'Predicted Spam'],\n            index = ['Actual Ham', 'Actual Spam'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6d74d2868237cbb5fa1fb6b190c1b4bf8b2d1e67"},"cell_type":"code","source":"# Setting some information that will be useful\n\nTP = n_confusion_test[0][0]\nTN = n_confusion_test[1][1]\nFP = n_confusion_test[1][0]\nFN = n_confusion_test[0][1]\nTrues = TP + TN\nFalses = FP + FN\nAll = TP + TN + FP + FN\nrecall = TP / (TP + FN)\nprecision = TP / (TP + FP)\nspecificity = TN / (TN + FP)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3422e212a3e3bc1a921f8f85399394b8d9c5b99d"},"cell_type":"markdown","source":" - Then, let's calculate our F-score with beta equal to 3 (it means that we consider recall 3 times more important than precision). In statistical analysis of binary classification, the F3 score (also F-score or F-measure) is a measure of a test's accuracy. It is the harmonic average of the precision and recall, where an F3 score reaches its best value at 1 (perfect precision and recall) and worst at 0. "},{"metadata":{"trusted":false,"_uuid":"2d320f3e6ac5d96d9f99cbe8558d95bdfa760fd7"},"cell_type":"code","source":"F3_score = 10 * precision * recall / (9 * precision + recall)\nprint ('F3 Score: ' + str(F3_score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afe794d67b8ff2dc4f89bb23af56c65a624d5088"},"cell_type":"markdown","source":" - In this step, we will plot the ROC Curve. (It is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varies. It's analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way to cost/benefit analysis of diagnostic decision making.)"},{"metadata":{"trusted":false,"_uuid":"ab4c4b43190923833f0451b0d020217d54a0c22d"},"cell_type":"code","source":"# Calculate the Fpr and Tpr for all thresholds of the classification\n\nfpr, tpr, threshold = metrics.roc_curve(Ytrain, YtrainPred)\nroc_auc = metrics.auc(fpr, tpr)\n\n# Ploting the (ROC Curve)\n\nplt.title('Receiver Operating Characteristic (The ROC cruve)')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b6528746827cf2dda501d3461523defc4e928d1"},"cell_type":"markdown","source":"## 5. Conclusions\n\n - After we analyze all the data and look at two methods of classification, we have some conclusions."},{"metadata":{"_uuid":"23e3af2809195782b44ee7ed10990dbd99f713b1"},"cell_type":"markdown","source":" 1. As we said in our initial data exploration, we need to find a way to our program get a classifier with an error less than 38% (because, in the worst case of telling that every email is HAM, we have 38% of error).\n 2. Comparing the Naive Bayes and K-NN, we concluded that (in this especific case) the Naive Bayes performs better than K-NN. The F3 score of Naive Bayes was around 0.932 (while the KNN performs 0.624). Moreover, the accuracy (0.825/0.742) and others rates were, generally, better in the Naive Bayes.\n 3. It is worth pointing out that not necessarily the Naive Bayes is the best possible method to be used in this case, however, compared to K-NN, he is the best.\n 4. There are more possible manipulations that could improve our classifier, as we saw in the Initial data exploration(2), a lot of information could be added to our classifier. Futhermore, we could increase our classifier with an spell corrector (mostly informal email text will not have the correct spellings), for example, [this is one of the most used](https://github.com/phatpiglet/autocorrect) for correct the spelling. Also, the verification of the origin of the email could be a good information to increase in our classification, because, according to researches, most of the accounts that send spam emails is used only with this function."},{"metadata":{"_uuid":"7a44e3aabd63b1410deb8326f5f2852595528910"},"cell_type":"markdown","source":" - Now, let's predict in our test file and send the results"},{"metadata":{"trusted":false,"_uuid":"e995b042163824bd6cc046103808d17b991cc0da"},"cell_type":"code","source":"# Open the file that contains a sample how to send ours predictions\n\nsample = pd.read_csv(\"../input/pmrdatasettarefa/sample_submission_1.csv\")\nprint (\"submission size: \", sample.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cec8205ae2181ecfcf5cad010120ded314401cec"},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"46c0f126f51bd9f1414cccf1c0fd591c663ccdc8"},"cell_type":"code","source":"# Predicting and sending the predictions\n\npredictionsTOsubmit = gnb.predict(testTOsubmit)\nstr(predictionsTOsubmit)\nids = testTOsubmit['Id']\nsubmission = pd.DataFrame({'Id':ids,'ham':predictionsTOsubmit[:]})\nsubmission.to_csv(\"predictions.csv\", index = False)\nsubmission.shape\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e30e79f51606593fb0bc75bec59749ae8ae0964"},"cell_type":"markdown","source":"## References\n\n\n - [Cross Validation Score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n - [Confusion Matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n - [K-Nearest Neighbors](http://scikit-learn.org/stable/modules/neighbors.html)\n - [Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html)\n - [Pandas](https://pandas.pydata.org/pandas-docs/stable/)\n - [ROC Curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n - [Split Data](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)\n "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}